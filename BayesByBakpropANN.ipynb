{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BayesByBakpropANN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "p93K3tzHzhLk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "7225fb5a-0aad-437f-8db7-39dab9c69999"
      },
      "source": [
        "\"\"\"\n",
        "Mounts your drive for reading and writing datasets, results, weights.\n",
        "\"\"\"\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSXLZt_GqW19",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import struct\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "def one_hot(a, num_classes):\n",
        "  \"\"\"\n",
        "  Converts the input numpy array to one hot.\n",
        "\n",
        "  Parameters\n",
        "  ==========\n",
        "\n",
        "  a: numpy.array\n",
        "    The input array.\n",
        "  num_classes: int\n",
        "    The number of classes to be used for one hot notation.\n",
        "  \n",
        "  Returns\n",
        "  =======\n",
        "\n",
        "  np.array\n",
        "  \"\"\"\n",
        "  return np.squeeze(np.eye(num_classes, dtype=np.float32)[a.reshape(-1)])\n",
        "\n",
        "def read_idx(filename, flatten=True, normalize=True, show_logs=True,\n",
        "             num_data=1000000, to_one_hot=False,\n",
        "             cache_result=False, use_cache=False):\n",
        "    \"\"\"\n",
        "    Reads datasets from binary files.\n",
        "\n",
        "    filename: str\n",
        "      The path to the file where the dataset is stored.\n",
        "      Make sure to not add any extensions at the end of\n",
        "      the filepath.\n",
        "    flatten: bool\n",
        "      If True, flattens the data points. By default, True.\n",
        "    normalize: bool\n",
        "      If True, the data points are normalised\n",
        "      by dividing each element by 126. By default, True.\n",
        "    show_logs: bool\n",
        "      If True, shows the logs while the data is\n",
        "      being processed. By default, True.\n",
        "    num_data: int\n",
        "      Number of data points to be fetched from the \n",
        "      dataset. By default, 1000000.\n",
        "    to_one_hot: bool\n",
        "      If True, converts data points to one hot \n",
        "      notation using 0 for low and 1 for high.\n",
        "      By default, False.\n",
        "    cache_result: bool\n",
        "      If True, caches the pre-processed data to \n",
        "      cache files of `npy` format. By default, False.\n",
        "    use_cache: bool\n",
        "      If True, uses the previously cached data, if possible.\n",
        "      By default, False.\n",
        "    \n",
        "    Returns\n",
        "    =======\n",
        "\n",
        "    np.array\n",
        "\n",
        "    Note\n",
        "    ====\n",
        "\n",
        "    This function was specifically written for MNIST database\n",
        "    of handwritten digits.\n",
        "    \"\"\"\n",
        "    if use_cache:\n",
        "        filename = filename + \"_cache.npy\"\n",
        "        ret_val = np.load(filename)\n",
        "        print(\"Loaded cached data from %s\"%(filename))\n",
        "        return ret_val\n",
        "    with open(filename, 'rb') as f:\n",
        "        zero, data_type, dims = struct.unpack('>HBB', f.read(4))\n",
        "        shape = tuple(struct.unpack('>I', f.read(4))[0] for d in range(dims))\n",
        "        ret_val = np.fromstring(f.read(), dtype=np.uint8).reshape(shape)\n",
        "        num_data = min(ret_val.shape[0], num_data)\n",
        "        normalize_val = []\n",
        "        if normalize:\n",
        "            for i in range(num_data):\n",
        "                mat = [[0 for _i in range(ret_val.shape[1])]\n",
        "                        for _j in range(ret_val.shape[2])]\n",
        "                if show_logs:\n",
        "                    print(\"Normalized %s-th data\"%(i+1))\n",
        "                for j in range(ret_val.shape[1]):\n",
        "                    for k in range(ret_val.shape[2]):\n",
        "                        mat[j][k] = ret_val[i][j][k]/126.\n",
        "                normalize_val.append(mat)\n",
        "                del mat\n",
        "            ret_val = np.asarray(normalize_val, dtype=np.float32)\n",
        "        del normalize_val\n",
        "        flatten_val = []\n",
        "        if flatten:\n",
        "            for i in range(num_data):\n",
        "                if show_logs:\n",
        "                    print(\"Flattened %s-th data\"%(i+1))\n",
        "                flatten_val.append(ret_val[i].flatten('C'))\n",
        "            ret_val = np.asarray(flatten_val, dtype=np.float32)\n",
        "        del flatten_val\n",
        "        if to_one_hot:\n",
        "            ret_val = one_hot(ret_val[0:num_data], 10)\n",
        "        if cache_result:\n",
        "            np.save(filename+\"_cache\", ret_val)\n",
        "            print(\"Saved cached data to %s_cache\"%(filename))\n",
        "        return ret_val"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDg7ma_8rGs1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "class ANNLayer(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Base class for ANN layers.\n",
        "\n",
        "    Parameters\n",
        "    ==========\n",
        "\n",
        "    num_inputs: int\n",
        "        Number of inputs to the layer.\n",
        "    num_outputs: int\n",
        "        Number of outputs from the layer.\n",
        "    activation:\n",
        "        Activation from tensorflow.keras.activations.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_inputs, num_outputs, activation):\n",
        "        super(ANNLayer, self).__init__(dtype=tf.float32)\n",
        "        self.num_outputs = num_outputs\n",
        "        self.activation = activation\n",
        "        self.kernel_mu = self.add_variable(\"kernel_mu\",\n",
        "                                            shape=[num_inputs,\n",
        "                                                   self.num_outputs],\n",
        "                                            initializer=tf.keras.initializers.TruncatedNormal(),\n",
        "                                            dtype=tf.float32)\n",
        "\n",
        "    def call(self, input):\n",
        "        \"\"\"\n",
        "        Implements the feed forward operation when layer \n",
        "        is called on the given input and weights.\n",
        "\n",
        "        input: tf.Tensor\n",
        "          The input.\n",
        "        \n",
        "        Returns\n",
        "        =======\n",
        "\n",
        "        tf.Tensor\n",
        "        \"\"\"\n",
        "        prod = self.activation(tf.matmul(input, self.kernel_mu))\n",
        "        return prod\n",
        "\n",
        "class BNNLayer(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Base class for BNN layers.\n",
        "\n",
        "    Parameters\n",
        "    ==========\n",
        "\n",
        "    num_inputs: int\n",
        "        Number of inputs to the layer.\n",
        "    num_outputs: int\n",
        "        Number of outputs from the layer.\n",
        "    activation:\n",
        "        Activation from tensorflow.keras.activations.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_inputs, num_outputs, activation):\n",
        "        super(BNNLayer, self).__init__(dtype=tf.float32)\n",
        "        self.num_outputs = num_outputs\n",
        "        self.activation = activation\n",
        "        self.kernel_mu = self.add_variable(\"kernel_mu\",\n",
        "                                            shape=[num_inputs,\n",
        "                                                   self.num_outputs],\n",
        "                                            initializer=tf.keras.initializers.TruncatedNormal(),\n",
        "                                            dtype=tf.float32)\n",
        "        self.kernel_rho = self.add_variable(\"kernel_sigma\",\n",
        "                                            shape=[num_inputs,\n",
        "                                                   self.num_outputs],\n",
        "                                            initializer=tf.keras.initializers.TruncatedNormal(),\n",
        "                                            dtype=tf.float32)\n",
        "\n",
        "    def _reparametrize(self):\n",
        "        \"\"\"\n",
        "        Abstract method which implements the\n",
        "        reparametrisation trick.\n",
        "        \"\"\"\n",
        "        return None\n",
        "\n",
        "    def call(self, input, weights):\n",
        "        \"\"\"\n",
        "        Implements the inference operation when layer \n",
        "        is called on the given input and weights.\n",
        "\n",
        "        input: tf.Tensor\n",
        "          The input.\n",
        "        weights: tf.Tensor\n",
        "          The weights\n",
        "        \n",
        "        Returns\n",
        "        =======\n",
        "\n",
        "        tf.Tensor\n",
        "        \"\"\"\n",
        "        prod = self.activation(tf.matmul(input, weights))\n",
        "        return prod\n",
        "\n",
        "class BNNLayer_Normal_Normal(BNNLayer):\n",
        "    \"\"\"\n",
        "    BNN layer which implements reparametrisation\n",
        "    trick from N(0, 1) to any N(mu, sigma).\n",
        "    \"\"\"\n",
        "\n",
        "    def _reparametrize(self):\n",
        "        eps_w_shape = self.kernel_mu.shape\n",
        "        eps_w = tf.random.normal(eps_w_shape, 0, 0.01, dtype=tf.float32)\n",
        "        term_w = tf.math.multiply(eps_w,\n",
        "                                  tf.math.log(tf.math.add(\n",
        "                                  tf.math.exp(tf.clip_by_value(self.kernel_rho, -87.315, 88.722)),\n",
        "                                  tf.constant(1., shape=eps_w_shape, dtype=tf.float32))))\n",
        "        return tf.math.add(self.kernel_mu, term_w)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-24sIjZrRCg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.activations import relu as Relu, elu as Elu, softmax as Softmax\n",
        "import math\n",
        "\n",
        "class ANN(tf.keras.Model):\n",
        "    \"\"\"\n",
        "    Artificial Neural Network using point estimates\n",
        "    of underlying distribution of training data.\n",
        "\n",
        "    Parameters\n",
        "    ==========\n",
        "\n",
        "    input_shape: tuple\n",
        "      By default, None.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_shape=None):\n",
        "        super(ANN, self).__init__()\n",
        "        self.InputLayer = tf.keras.layers.InputLayer(input_shape=(input_shape[1],),\n",
        "                            batch_size=input_shape[0], dtype=tf.float32)\n",
        "        self.Dense_1 = ANNLayer(int(input_shape[-1]), 400, activation=Relu)\n",
        "        self.Dense_2 = ANNLayer(400, 400, activation=Relu)\n",
        "        self.Output = ANNLayer(400, 10, activation=Softmax)\n",
        "        self.Layers = [self.Dense_1, self.Dense_2, self.Output]\n",
        "\n",
        "    def run(self, inputs):\n",
        "        \"\"\"\n",
        "        Performs feed forward operation on the given inputs.\n",
        "\n",
        "        Parameters\n",
        "        ==========\n",
        "\n",
        "        inputs: tf.Tensor\n",
        "\n",
        "        Returns\n",
        "        =======\n",
        "\n",
        "        tf.Tensor\n",
        "        \"\"\"\n",
        "        layer_output = self.InputLayer(inputs)\n",
        "        i = 0\n",
        "        for layer in self.Layers:\n",
        "            layer_output = layer(layer_output)\n",
        "            i += 1\n",
        "        return layer_output\n",
        "\n",
        "    def get_loss(self, inputs, targets, inference=False):\n",
        "        \"\"\"\n",
        "        Computes the total training loss.\n",
        "\n",
        "        Parameters\n",
        "        ==========\n",
        "\n",
        "        inputs: tf.Tensor\n",
        "            Input to the layers.\n",
        "        targets: tf.Tensor\n",
        "            True targets that the model wants to learn from.\n",
        "        inference: bool\n",
        "            Used to determine the order of the outputs in the tuple\n",
        "            being returned.\n",
        "        \n",
        "        Returns\n",
        "        =======\n",
        "\n",
        "        tuple\n",
        "          Containing loss and output of neural network for each sample.\n",
        "        \"\"\"\n",
        "        outputs = self.run(inputs)\n",
        "        loss = tf.reduce_mean(tf.keras.losses.categorical_crossentropy(targets, outputs))\n",
        "\n",
        "        if inference:\n",
        "            return outputs, loss\n",
        "        return loss, outputs\n",
        "\n",
        "    def compute_gradients(self, inputs, targets):\n",
        "        \"\"\"\n",
        "        Computes gradients of cost function for given inputs, \n",
        "        targets.\n",
        "\n",
        "        Parameters\n",
        "        ==========\n",
        "\n",
        "        inputs: tf.Tensor\n",
        "        targets: tf.Tensor\n",
        "        weight: tf.float32 or equivalent\n",
        "          The weight given to each batch of input data.\n",
        "        \n",
        "        Returns\n",
        "        =======\n",
        "\n",
        "        list\n",
        "          Containing gradients, tf.Tensor, w.r.t each variable.\n",
        "        \"\"\"\n",
        "        with tf.GradientTape(persistent=True) as tape:\n",
        "            _vars = []\n",
        "            for layer in self.Layers:\n",
        "                _vars.append(layer.kernel_mu)\n",
        "            tape.watch(_vars)\n",
        "\n",
        "            F, _ = self.get_loss(inputs, targets)\n",
        "            dF = tape.gradient(F, _vars)\n",
        "\n",
        "        return dF\n",
        "\n",
        "    def learn(self, inputs, targets, alpha):\n",
        "        \"\"\"\n",
        "        Performs weight updates.\n",
        "\n",
        "        Parameters\n",
        "        ==========\n",
        "\n",
        "        inputs: tf.Tensor\n",
        "        targets: tf.Tensor\n",
        "        alpha: tf.float32 or equivalent\n",
        "          The learning rate.\n",
        "        \n",
        "        Returns\n",
        "        =======\n",
        "\n",
        "        None\n",
        "        \"\"\"\n",
        "        grads = self.compute_gradients(inputs, targets)\n",
        "        i = 0\n",
        "        for layer in self.Layers:\n",
        "            layer.kernel_mu.assign(tf.math.subtract(layer.kernel_mu, tf.scalar_mul(alpha, grads[i])))\n",
        "            i += 1\n",
        "\n",
        "class BNN_Normal_Normal(tf.keras.Model):\n",
        "    \"\"\"\n",
        "    Neural Network which uses, `BNNLayer_Normal_Normal` layers.\n",
        "\n",
        "    Parameters\n",
        "    ==========\n",
        "\n",
        "    input_shape: tuple\n",
        "      By default, None.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_shape=None):\n",
        "        super(BNN_Normal_Normal, self).__init__()\n",
        "        self.InputLayer = tf.keras.layers.InputLayer(input_shape=(input_shape[1],),\n",
        "                            batch_size=input_shape[0], dtype=tf.float32)\n",
        "        self.Dense_1 = BNNLayer_Normal_Normal(int(input_shape[-1]), 400, activation=Relu)\n",
        "        self.Dense_2 = BNNLayer_Normal_Normal(400, 400, activation=Relu)\n",
        "        self.Output = BNNLayer_Normal_Normal(400, 10, activation=Softmax)\n",
        "        self.Layers = [self.Dense_1, self.Dense_2, self.Output]\n",
        "\n",
        "    def run(self, inputs, *weights):\n",
        "        \"\"\"\n",
        "        Produces neural network's outputs for\n",
        "        given inputs and weights.\n",
        "\n",
        "        Parameters\n",
        "        ==========\n",
        "\n",
        "        inputs: tf.Tensor/np.array\n",
        "        weights: tf.Tensor\n",
        "\n",
        "        Returns\n",
        "        =======\n",
        "\n",
        "        tf.Tensor\n",
        "        \"\"\"\n",
        "        layer_output = self.InputLayer(inputs)\n",
        "        i = 0\n",
        "        for layer in self.Layers:\n",
        "            layer_output = layer(layer_output, weights[i])\n",
        "            i += 1\n",
        "        return layer_output\n",
        "\n",
        "    def log_prior(self, weights):\n",
        "        \"\"\"\n",
        "        Computes the natural logarithm of scale\n",
        "        mixture prior of weights.\n",
        "\n",
        "        Parameters\n",
        "        ==========\n",
        "\n",
        "        weights: tf.Tensor\n",
        "\n",
        "        Returns\n",
        "        =======\n",
        "\n",
        "        tf.Tensor\n",
        "\n",
        "        Note\n",
        "        ====\n",
        "\n",
        "        The two standard deviations of the scale mixture are,\n",
        "        exp(0) and exp(-6). The weight of both normal distributions\n",
        "        is 0.5.\n",
        "        \"\"\"\n",
        "        shape = weights.shape\n",
        "        sigma_1 = tf.constant(math.exp(0), shape=shape, dtype=tf.float32)\n",
        "        sigma_2 = tf.constant(math.exp(-6), shape=shape, dtype=tf.float32)\n",
        "        def pdf(w, sigma):\n",
        "            res1 = tf.math.divide(tf.math.square(w), tf.math.square(sigma)*2)\n",
        "            return tf.math.divide(tf.math.exp(tf.clip_by_value(-res1, -87.315, 88.722)), sigma*(2*math.pi)**0.5)\n",
        "        part_1 = tf.clip_by_value(0.25*pdf(weights, sigma_1), tf.float32.min//2, tf.float32.max//2)\n",
        "        part_2 = tf.clip_by_value(0.75*pdf(weights, sigma_2), tf.float32.min//2, tf.float32.max//2)\n",
        "        return tf.math.reduce_sum(tf.math.log(part_1 + part_2))\n",
        "\n",
        "    def log_posterior(self, weights, mu, rho):\n",
        "        \"\"\"\n",
        "        Computes the natural logarithm of Gaussian\n",
        "        posterior on weights.\n",
        "\n",
        "        Parameters\n",
        "        ==========\n",
        "\n",
        "        weights: tf.Tensor\n",
        "        mu: tf.Tensor\n",
        "          The mean of the posterior Gaussian distribution.\n",
        "        rho: tf.Tensor\n",
        "          Used to compute the variance of the posterior Gaussian distribution.\n",
        "        \n",
        "        Returns\n",
        "        =======\n",
        "\n",
        "        tf.Tensor\n",
        "        \"\"\"\n",
        "        def pdf(w, mu, sigma):\n",
        "            res1 = tf.math.divide(tf.math.square(w - mu), tf.math.square(sigma)*2)\n",
        "            return tf.math.divide(tf.math.exp(tf.clip_by_value(-res1, -87.315, 88.722)), sigma*(2*math.pi)**0.5)\n",
        "        sigma = tf.math.log(tf.math.add(\n",
        "                                  tf.math.exp(tf.clip_by_value(rho, -87.315, 88.722)),\n",
        "                                  tf.constant(1., shape=rho.shape, dtype=tf.float32)))\n",
        "        log_q = tf.math.log(tf.clip_by_value(pdf(weights, mu, sigma), tf.float32.min//2, tf.float32.max//2))\n",
        "        return tf.math.reduce_sum(log_q)\n",
        "\n",
        "    def get_loss(self, inputs, targets, samples, weight=1., inference=False):\n",
        "        \"\"\"\n",
        "        Computes the total training loss.\n",
        "\n",
        "        Parameters\n",
        "        ==========\n",
        "\n",
        "        inputs: tf.Tensor/np.array\n",
        "            Input to the layers.\n",
        "        targets: tf.Tensor/np.array\n",
        "            True targets that the model wants to learn from.\n",
        "        samples: int\n",
        "            The number of samples to be drawn for weights.\n",
        "        weight: tf.float32 or equivalent.\n",
        "            Weight given to loss of each batch. By default, 1.\n",
        "        inference: bool\n",
        "            Used to determine the order of the outputs in the tuple\n",
        "            being returned.\n",
        "        \n",
        "        Returns\n",
        "        =======\n",
        "\n",
        "        tuple\n",
        "          Containing loss and output of neural network for each sample.\n",
        "        \"\"\"\n",
        "        loss = tf.constant(0., dtype=tf.float32)\n",
        "        outputs_list = []\n",
        "        for _ in range(samples):\n",
        "            weights = []\n",
        "            pw, qw = tf.constant(0, dtype=tf.float32), tf.constant(0, dtype=tf.float32)\n",
        "            for layer in self.Layers:\n",
        "                weights.append(layer._reparametrize())\n",
        "                kernel_mu, kernel_rho = layer.kernel_mu, layer.kernel_rho\n",
        "                pw += self.log_prior(weights[-1])\n",
        "                qw += self.log_posterior(weights[-1], kernel_mu, kernel_rho)\n",
        "\n",
        "            outputs = self.run(inputs, *weights)\n",
        "            outputs_list.append(outputs)\n",
        "            cse = tf.reduce_mean(tf.keras.losses.categorical_crossentropy(targets, outputs))\n",
        "            if inference:\n",
        "                loss += cse\n",
        "            else:\n",
        "                loss += (qw - pw)*weight + tf.cast(cse, tf.float32)\n",
        "\n",
        "        if inference:\n",
        "            return outputs_list, loss/samples\n",
        "        return loss/samples, outputs_list\n",
        "\n",
        "    def compute_gradients(self, inputs, targets, weight):\n",
        "        \"\"\"\n",
        "        Computes gradients of cost function for given inputs, \n",
        "        targets.\n",
        "\n",
        "        Parameters\n",
        "        ==========\n",
        "\n",
        "        inputs: tf.Tensor\n",
        "        targets: tf.Tensor\n",
        "        weight: tf.float32 or equivalent\n",
        "          The weight given to each batch of input data.\n",
        "        \n",
        "        Returns\n",
        "        =======\n",
        "\n",
        "        list\n",
        "          Containing gradients, tf.Tensor, w.r.t each variable.\n",
        "        \"\"\"\n",
        "        with tf.GradientTape(persistent=True) as tape:\n",
        "            _vars = []\n",
        "            for layer in self.Layers:\n",
        "                _vars.append(layer.kernel_rho)\n",
        "            tape.watch(_vars)\n",
        "\n",
        "            F, _ = self.get_loss(inputs, targets, 10, weight)\n",
        "            dF = tape.gradient(F, _vars)\n",
        "\n",
        "        return dF\n",
        "\n",
        "    def learn(self, inputs, targets, alpha, weight=1.):\n",
        "        \"\"\"\n",
        "        Performs parameter updates.\n",
        "\n",
        "        Parameters\n",
        "        ==========\n",
        "\n",
        "        inputs: tf.Tensor\n",
        "        targets: tf.Tensor\n",
        "        alpha: tf.float32 or equivalent\n",
        "          The learning rate.\n",
        "        weight: tf.float32 or equivalent\n",
        "          The weight given to each batch of input data.\n",
        "        \n",
        "        Returns\n",
        "        =======\n",
        "\n",
        "        None\n",
        "        \"\"\"\n",
        "        grads = self.compute_gradients(inputs, targets, weight)\n",
        "        i = 0\n",
        "        for layer in self.Layers:\n",
        "            layer.kernel_rho.assign(tf.math.subtract(layer.kernel_rho, tf.scalar_mul(alpha, grads[i])))\n",
        "            i += 1\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9pVNPQcrZ4G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from datetime import datetime\n",
        "\n",
        "prefix = \"./drive/My Drive/bnn/\"\n",
        "\n",
        "def random_shuffle(x, y):\n",
        "    \"\"\"\n",
        "    Shuffles the dataset.\n",
        "\n",
        "    Parameters\n",
        "    ==========\n",
        "\n",
        "    x: np.array\n",
        "      The input dataset.\n",
        "    y: np.array\n",
        "      The target labels.\n",
        "    \n",
        "    Returns\n",
        "    =======\n",
        "\n",
        "    tuple\n",
        "      First element contains x and the second y\n",
        "      both shuffled.\n",
        "    \"\"\"\n",
        "    indices = np.arange(x.shape[0])\n",
        "    np.random.shuffle(indices)\n",
        "    return x[indices], y[indices]\n",
        "\n",
        "def take_subset(x, y, size, count):\n",
        "    \"\"\"\n",
        "    Takes subset of inputs.\n",
        "\n",
        "    Parameters\n",
        "    ==========\n",
        "\n",
        "    x: np.array\n",
        "      The input dataset.\n",
        "    y: np.array\n",
        "      The target labels.\n",
        "    size: int\n",
        "      The size of the subset\n",
        "    count: int\n",
        "      To be used for finding the initial\n",
        "      posistion of the subset.\n",
        "\n",
        "    Returns\n",
        "    =======\n",
        "\n",
        "    tuple\n",
        "      Both np.array elements.    \n",
        "    \"\"\"\n",
        "    return x[count*size:count*size + size], \\\n",
        "           y[count*size:count*size + size]\n",
        "\n",
        "def train(input_data_path, target_data_path, checkpoint=None, \n",
        "          alpha=0.01, batch_size=1, dataset_size=1,\n",
        "          epochs=1, cache_result=True, use_cache=False):\n",
        "    \"\"\"\n",
        "    Trains the neural network, `ANN` and saves it's weights under, `bnn/<data-set>/weights`.\n",
        "\n",
        "    Parameters\n",
        "    ==========\n",
        "\n",
        "    input_data_path: str\n",
        "      The path to the file containing input data. Must be binary.\n",
        "    target_data_path: str\n",
        "      The path to the file containing target label data. Must be binary.\n",
        "    checkpoint: str\n",
        "      The name of the file from the checkpoint is to be loaded.\n",
        "      By default, None.\n",
        "    alpha: tf.float32 or equivalent\n",
        "      The learning rate.\n",
        "    batch_size: int\n",
        "      By default, 1.\n",
        "    dataset_size: int\n",
        "      By default, 1.\n",
        "    epochs: int\n",
        "      By default 1.\n",
        "    cache_result: bool\n",
        "      If True, caches the pre-processed data to \n",
        "      cache files of `npy` format. By default, True.\n",
        "    use_cache: bool\n",
        "      If True, uses the previously cached data, if possible.\n",
        "      By default, False.\n",
        "    \n",
        "    Returns\n",
        "    =======\n",
        "\n",
        "    None\n",
        "    \"\"\"\n",
        "    curr_date_time = str(datetime.now()).replace(' ', '_').replace(':', '_').replace('-', '_').replace('.', '_')\n",
        "    if checkpoint is not None:\n",
        "        curr_date_time = checkpoint\n",
        "        logs = open(prefix + \"logs/_\"+curr_date_time, 'a')\n",
        "    else:\n",
        "        logs = open(prefix + \"logs/_\"+curr_date_time, 'w+')\n",
        "    inputs = read_idx(input_data_path, True, True, True, dataset_size, False, cache_result, use_cache)\n",
        "    targets = read_idx(target_data_path, False, False, True, dataset_size, True, cache_result, use_cache)\n",
        "    logs.write(str((inputs.shape, targets.shape)) + '\\n')\n",
        "    size = targets.shape[0]\n",
        "    model = ANN(input_shape=(batch_size, 784))\n",
        "    if checkpoint is not None:\n",
        "        model.load_weights(prefix + \"weights/mnist/_\" + checkpoint)\n",
        "    logs.write(\"Number of batches per epoch: \" + str(size//batch_size) + '\\n')\n",
        "    logs.write(\"Number of epochs: \" + str(epochs) + '\\n')\n",
        "    logs.write(\"Initial Loss: \" + str(model.get_loss(inputs, targets)[0]) + '\\n')\n",
        "    logs.close()\n",
        "    logs = open(prefix + \"logs/_\"+curr_date_time, 'a')\n",
        "    for epoch in range(epochs):\n",
        "        inputs, targets = random_shuffle(inputs, targets)\n",
        "        for batch in range(size//batch_size):\n",
        "            input_sub, target_sub = take_subset(inputs, targets, batch_size, batch)\n",
        "            model.learn(input_sub, target_sub, alpha)\n",
        "        logs.write(\"Loss at completion of epoch \" + str(epoch) + \" is \" + str(model.get_loss(inputs, targets)[0]) + '\\n')\n",
        "        if epoch%2 == 0:\n",
        "            model.save_weights(prefix + \"weights/mnist/_\" + curr_date_time)\n",
        "            logs.close()\n",
        "            logs = open(prefix + \"logs/_\" + curr_date_time, 'a')\n",
        "    logs.close()\n",
        "\n",
        "def train_bnn(input_data_path, target_data_path, ann_weights, checkpoint=None, \n",
        "              alpha=0.01, batch_size=1, dataset_size=1,\n",
        "              epochs=1, cache_result=True, use_cache=False):\n",
        "    \"\"\"\n",
        "    Trains the neural network, `BNN_Normal_Normal` and saves it's parameters under, `bnn/<data-set>/weights`.\n",
        "\n",
        "    Parameters\n",
        "    ==========\n",
        "\n",
        "    input_data_path: str\n",
        "      The path to the file containing input data. Must be binary.\n",
        "    target_data_path: str\n",
        "      The path to the file containing target label data. Must be binary.\n",
        "    ann_weights: str\n",
        "      The path to the file which contains weights of a pre-trained ANN\n",
        "      whose weights will be used as means of the Gaussian distributions\n",
        "      of the weights of the new neural network.\n",
        "    checkpoint: str\n",
        "      The name of the file from the checkpoint is to be loaded.\n",
        "      By default, None.\n",
        "    alpha: tf.float32 or equivalent\n",
        "      The learning rate.\n",
        "    batch_size: int\n",
        "      By default, 1.\n",
        "    dataset_size: int\n",
        "      By default, 1.\n",
        "    epochs: int\n",
        "      By default 1.\n",
        "    cache_result: bool\n",
        "      If True, caches the pre-processed data to \n",
        "      cache files of `npy` format. By default, True.\n",
        "    use_cache: bool\n",
        "      If True, uses the previously cached data, if possible.\n",
        "      By default, False.\n",
        "    \n",
        "    Returns\n",
        "    =======\n",
        "\n",
        "    None\n",
        "    \"\"\"\n",
        "    curr_date_time = str(datetime.now()).replace(' ', '_').replace(':', '_').replace('-', '_').replace('.', '_')\n",
        "    if checkpoint is not None:\n",
        "        curr_date_time = checkpoint\n",
        "        logs = open(prefix + \"logs/\"+curr_date_time, 'a')\n",
        "    else:\n",
        "        logs = open(prefix + \"logs/\"+curr_date_time, 'w+')\n",
        "    inputs = read_idx(input_data_path, True, True, True, dataset_size, False, cache_result, use_cache)\n",
        "    targets = read_idx(target_data_path, False, False, True, dataset_size, True, cache_result, use_cache)\n",
        "    logs.write(str((inputs.shape, targets.shape)) + '\\n')\n",
        "    size = targets.shape[0]\n",
        "    model = BNN_Normal_Normal(input_shape=(batch_size, 784))\n",
        "    if checkpoint is not None:\n",
        "        model.load_weights(prefix + \"weights/mnist/\" + checkpoint)\n",
        "    else:\n",
        "        modelANN = ANN(input_shape=(None, 784))\n",
        "        modelANN.load_weights(ann_weights)\n",
        "        for layerANN, layerBNN in zip(modelANN.Layers, model.Layers):\n",
        "          layerBNN.kernel_mu.assign(layerANN.kernel_mu)\n",
        "    logs.write(\"Number of batches per epoch: \" + str(size//batch_size) + '\\n')\n",
        "    logs.write(\"Number of epochs: \" + str(epochs) + '\\n')\n",
        "    logs.write(\"Initial Loss: \" + str(model.get_loss(inputs, targets, 1)[0]) + '\\n')\n",
        "    logs.close()\n",
        "    logs = open(prefix + \"logs/\"+curr_date_time, 'a')\n",
        "    for epoch in range(epochs):\n",
        "        inputs, targets = random_shuffle(inputs, targets)\n",
        "        for batch in range(size//batch_size):\n",
        "            input_sub, target_sub = take_subset(inputs, targets, batch_size, batch)\n",
        "            model.learn(input_sub, target_sub, alpha, 1/(size//batch_size))\n",
        "        logs.write(\"Loss at completion of epoch \" + str(epoch) + \" is \" + str(model.get_loss(inputs, targets, 10)[0]) + '\\n')\n",
        "        logs.write(\"Cross entropy at completion of epoch \" + str(epoch) + \" is \" + str(model.get_loss(inputs, targets, 1, 1., True)[1]) + '\\n')\n",
        "        if epoch%2 == 0:\n",
        "            model.save_weights(prefix + \"weights/mnist/\" + curr_date_time)\n",
        "            logs.close()\n",
        "            logs = open(prefix + \"logs/\" + curr_date_time, 'a')\n",
        "    logs.close()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87dMtwC9ri6l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "outputId": "ee8f17e9-2290-4b81-dded-966996f2d03f"
      },
      "source": [
        "\"\"\"\n",
        "Training `ANN` on MNIST database of handwritten digits.\n",
        "\"\"\"\n",
        "train(prefix + \"datasets/mnist_train_images\", prefix + \"datasets/mnist_train_labels\", \n",
        "      None, 0.01, 20000, 60000, 100, False, True)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded cached data from ./drive/My Drive/bnn/datasets/mnist_train_images_cache.npy\n",
            "Loaded cached data from ./drive/My Drive/bnn/datasets/mnist_train_labels_cache.npy\n",
            "WARNING:tensorflow:From <ipython-input-3-1622adb82db3>:26: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.add_weight` method instead.\n",
            "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-f6670b677ace>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \"\"\"\n\u001b[1;32m      4\u001b[0m train(prefix + \"datasets/mnist_train_images\", prefix + \"datasets/mnist_train_labels\", \n\u001b[0;32m----> 5\u001b[0;31m       None, 0.01, 20000, 60000, 100, False, True)\n\u001b[0m",
            "\u001b[0;32m<ipython-input-5-66ab3a699897>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(input_data_path, target_data_path, checkpoint, alpha, batch_size, dataset_size, epochs, cache_result, use_cache)\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"logs/_\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mcurr_date_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'a'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_shuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0minput_sub\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_sub\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtake_subset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-66ab3a699897>\u001b[0m in \u001b[0;36mrandom_shuffle\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtake_subset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0147yJlfgG2R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(file_path, input_shape, input_data, output_data, cache_result, use_cache):\n",
        "    \"\"\"\n",
        "    Used for evaluating the trained neural network.\n",
        "\n",
        "    Parameters\n",
        "    ==========\n",
        "\n",
        "    file_path: str\n",
        "      The path to the file from where the saved `BNN_Normal_Normal` is to be loaded.\n",
        "    input_shape: tuple\n",
        "    input_data: str\n",
        "      The path to the file containing input data. Must be binary.\n",
        "    output_data: str\n",
        "      The path to the file containing output_data. Must be binary.\n",
        "    cache_result: bool\n",
        "      If True, caches the pre-processed data to \n",
        "      cache files of `npy` format. By default, True.\n",
        "    use_cache: bool\n",
        "      If True, uses the previously cached data, if possible.\n",
        "      By default, False.\n",
        "    \n",
        "    Returns\n",
        "    =======\n",
        "\n",
        "    Tuple of the following,\n",
        "\n",
        "    preds: list containing tf.Tensor\n",
        "      Predictions made by neural network.\n",
        "    targets: tf.Tensor\n",
        "      The target labels.\n",
        "    total_loss: tf.float32\n",
        "    \"\"\"\n",
        "    model = ANN(input_shape=input_shape)\n",
        "    model.load_weights(file_path)\n",
        "    inputs = read_idx(input_data, True, True, True, 10000, False, cache_result, use_cache)\n",
        "    targets = read_idx(output_data, False, False, True, 10000, True, cache_result, use_cache)\n",
        "    preds, total_loss = model.get_loss(inputs, targets, True)\n",
        "    return preds, targets, total_loss"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKrM8IDYgilY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463
        },
        "outputId": "f393bc6e-e6de-4413-b59a-7b2e0553e376"
      },
      "source": [
        "\"\"\"\n",
        "Training loop for benchmarking. \n",
        "This cell keeps on optimising the weights of the ANN until the test accuracy keeps on increasing.\n",
        "\"\"\"\n",
        "curr_acc = 0\n",
        "while True:\n",
        "  train(prefix + \"datasets/mnist_train_images\", prefix + \"datasets/mnist_train_labels\", \n",
        "        \"2020_08_02_06_33_12_410605\", 0.01, 20000, 60000, 25, False, True)\n",
        "  preds, targets, _ = test(prefix + \"weights/mnist/_2020_08_02_06_33_12_410605\", (None, 784), \n",
        "                           prefix + \"datasets/mnist_test_images\", prefix + \"datasets/mnist_test_labels\",\n",
        "                           False, True)\n",
        "  acc = 0\n",
        "  for pred, target in zip(preds, targets):\n",
        "    if tf.math.argmax(pred) == tf.math.argmax(target):\n",
        "      acc += 1\n",
        "  print(\"Accuracy: \", acc/100)\n",
        "  if acc >= curr_acc:\n",
        "    curr_acc = acc\n",
        "  else:\n",
        "    break"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded cached data from ./drive/My Drive/bnn/datasets/mnist_train_images_cache.npy\n",
            "Loaded cached data from ./drive/My Drive/bnn/datasets/mnist_train_labels_cache.npy\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-3431d24668be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   train(prefix + \"datasets/mnist_train_images\", prefix + \"datasets/mnist_train_labels\", \"2020_08_02_06_33_12_410605\", 0.01, 20000,\n\u001b[0;32m----> 8\u001b[0;31m       60000, 25, False, True)\n\u001b[0m\u001b[1;32m      9\u001b[0m   preds, targets, _ = test(prefix + \"weights/mnist/_2020_08_02_06_33_12_410605\", (None, 784), prefix + \"datasets/mnist_test_images\", prefix + \"datasets/mnist_test_labels\",\n\u001b[1;32m     10\u001b[0m       False, True)\n",
            "\u001b[0;32m<ipython-input-5-66ab3a699897>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(input_data_path, target_data_path, checkpoint, alpha, batch_size, dataset_size, epochs, cache_result, use_cache)\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0minput_sub\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_sub\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtake_subset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_sub\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_sub\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0mlogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loss at completion of epoch \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" is \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-8c02da8dc666>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, inputs, targets, alpha)\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \"\"\"\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-8c02da8dc666>\u001b[0m in \u001b[0;36mcompute_gradients\u001b[0;34m(self, inputs, targets)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0mdF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1071\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1072\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1073\u001b[0;31m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[1;32m   1074\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1075\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     75\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[1;32m    160\u001b[0m       \u001b[0mgradient_name_scope\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mforward_pass_name_scope\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient_name_scope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py\u001b[0m in \u001b[0;36m_MatMulGrad\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m   1679\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_MatMulGradAgainstFirstOnly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1680\u001b[0m       \u001b[0;32melif\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mskip_input_indices\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1681\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_MatMulGradAgainstSecondOnly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1682\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1683\u001b[0m     \u001b[0;31m# No gradient skipping, so do the full gradient computation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py\u001b[0m in \u001b[0;36m_MatMulGradAgainstSecondOnly\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m   1660\u001b[0m   \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1661\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mt_a\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mt_b\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1662\u001b[0;31m     \u001b[0mgrad_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmat_mul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranspose_a\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1663\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mt_a\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mt_b\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1664\u001b[0m     \u001b[0mgrad_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmat_mul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranspose_a\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mmat_mul\u001b[0;34m(a, b, transpose_a, transpose_b, name)\u001b[0m\n\u001b[1;32m   5619\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"MatMul\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5620\u001b[0m         \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop_callbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"transpose_a\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranspose_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"transpose_b\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5621\u001b[0;31m         transpose_b)\n\u001b[0m\u001b[1;32m   5622\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5623\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4b-yU6w89MU-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 414
        },
        "outputId": "81c19b09-60d1-4e81-aa3a-07953aa8f1d2"
      },
      "source": [
        "\"\"\"\n",
        "Training `BNN_Normal_Normal` on MNIST database of handwritten digits\n",
        "using the weights of a pre-trained `ANN`.\n",
        "\"\"\"\n",
        "train_bnn(prefix + \"datasets/mnist_train_images\", prefix + \"datasets/mnist_train_labels\", \n",
        "          prefix + \"weights/mnist/_2020_08_02_06_33_12_410605\", None, 0.01, 20000,\n",
        "          60000, 20, False, True)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded cached data from ./drive/My Drive/bnn/datasets/mnist_train_images_cache.npy\n",
            "Loaded cached data from ./drive/My Drive/bnn/datasets/mnist_train_labels_cache.npy\n",
            "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-76db66694662>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m train_bnn(prefix + \"datasets/mnist_train_images\", prefix + \"datasets/mnist_train_labels\", \n\u001b[1;32m      6\u001b[0m           \u001b[0mprefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"weights/mnist/_2020_08_02_06_33_12_410605\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m           60000, 20, False, True)\n\u001b[0m",
            "\u001b[0;32m<ipython-input-5-66ab3a699897>\u001b[0m in \u001b[0;36mtrain_bnn\u001b[0;34m(input_data_path, target_data_path, ann_weights, checkpoint, alpha, batch_size, dataset_size, epochs, cache_result, use_cache)\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[0minput_sub\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_sub\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtake_subset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_sub\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_sub\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m         \u001b[0mlogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loss at completion of epoch \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" is \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0mlogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cross entropy at completion of epoch \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" is \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-8c02da8dc666>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, inputs, targets, alpha, weight)\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m         \"\"\"\n\u001b[0;32m--> 327\u001b[0;31m         \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m         \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-8c02da8dc666>\u001b[0m in \u001b[0;36mcompute_gradients\u001b[0;34m(self, inputs, targets, weight)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m             \u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m             \u001b[0mdF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1071\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1072\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1073\u001b[0;31m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[1;32m   1074\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1075\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     75\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36m_aggregate_grads\u001b[0;34m(gradients)\u001b[0m\n\u001b[1;32m    641\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgradients\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgradients\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_n\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    644\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m     assert all(isinstance(g, (ops.Tensor, ops.IndexedSlices))\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36madd_n\u001b[0;34m(inputs, name)\u001b[0m\n\u001b[1;32m    412\u001b[0m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[1;32m    413\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"AddN\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop_callbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m         inputs)\n\u001b[0m\u001b[1;32m    415\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3siut4BCTDzN",
        "colab": {}
      },
      "source": [
        "def test(file_path, input_shape, input_data, output_data, cache_result, use_cache):\n",
        "    \"\"\"\n",
        "    Used for evaluating the trained neural network.\n",
        "\n",
        "    Parameters\n",
        "    ==========\n",
        "\n",
        "    file_path: str\n",
        "      The path to the file from where the saved `BNN_Normal_Normal` is to be loaded.\n",
        "    input_shape: tuple\n",
        "    input_data: str\n",
        "      The path to the file containing input data. Must be binary.\n",
        "    output_data: str\n",
        "      The path to the file containing output_data. Must be binary.\n",
        "    cache_result: bool\n",
        "      If True, caches the pre-processed data to \n",
        "      cache files of `npy` format. By default, True.\n",
        "    use_cache: bool\n",
        "      If True, uses the previously cached data, if possible.\n",
        "      By default, False.\n",
        "    \n",
        "    Returns\n",
        "    =======\n",
        "\n",
        "    Tuple of the following,\n",
        "\n",
        "    preds: list containing tf.Tensor\n",
        "      Predictions made by neural network. Shape (10, 10000).\n",
        "    targets: tf.Tensor\n",
        "      The target labels.\n",
        "    total_loss: tf.float32\n",
        "    \"\"\"\n",
        "    model = BNN_Normal_Normal(input_shape=input_shape)\n",
        "    model.load_weights(file_path)\n",
        "    inputs = read_idx(input_data, True, True, True, 10000, False, cache_result, use_cache)\n",
        "    targets = read_idx(output_data, False, False, True, 10000, True, cache_result, use_cache)\n",
        "    preds, total_loss = model.get_loss(inputs, targets, 10, 1., True)\n",
        "    return preds, targets, total_loss"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "o0j3xSsETKpA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "a9088abb-f44a-40e7-f5fe-a70db1f18de5"
      },
      "source": [
        "preds, targets, _ = test(prefix + \"weights/mnist/2020_08_09_08_27_21_674420\", (None, 784), \n",
        "                         prefix + \"datasets/mnist_test_images\", prefix + \"datasets/mnist_test_labels\",\n",
        "                         False, True)\n",
        "acc = 0\n",
        "for i in range(10):\n",
        "  for pred, target in zip(preds[i], targets):\n",
        "    if tf.math.argmax(pred) == tf.math.argmax(target):\n",
        "      acc += 1\n",
        "print(\"Accuracy: \", acc/(10*100))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded cached data from ./drive/My Drive/bnn/datasets/mnist_test_images_cache.npy\n",
            "Loaded cached data from ./drive/My Drive/bnn/datasets/mnist_test_labels_cache.npy\n",
            "Accuracy:  97.288\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}