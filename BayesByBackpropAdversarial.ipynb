{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BayesByBackpropAdversarial.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkIo_POZ_fFq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "fdd13cc4-0ca1-470d-ed49-51bd2ff9be6d"
      },
      "source": [
        "\"\"\"\n",
        "Mounts your drive for reading and writing datasets, results, weights.\n",
        "\"\"\"\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSXLZt_GqW19",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import struct\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "def one_hot(a, num_classes):\n",
        "  \"\"\"\n",
        "  Converts the input numpy array to one hot.\n",
        "\n",
        "  Parameters\n",
        "  ==========\n",
        "\n",
        "  a: numpy.array\n",
        "    The input array.\n",
        "  num_classes: int\n",
        "    The number of classes to be used for one hot notation.\n",
        "  \n",
        "  Returns\n",
        "  =======\n",
        "\n",
        "  np.array\n",
        "  \"\"\"\n",
        "  return np.squeeze(np.eye(num_classes, dtype=np.float32)[a.reshape(-1)])\n",
        "\n",
        "def read_idx(filename, flatten=True, normalize=True, show_logs=True,\n",
        "             num_data=1000000, to_one_hot=False,\n",
        "             cache_result=False, use_cache=False):\n",
        "    \"\"\"\n",
        "    Reads datasets from binary files.\n",
        "\n",
        "    filename: str\n",
        "      The path to the file where the dataset is stored.\n",
        "      Make sure to not add any extensions at the end of\n",
        "      the filepath.\n",
        "    flatten: bool\n",
        "      If True, flattens the data points. By default, True.\n",
        "    normalize: bool\n",
        "      If True, the data points are normalised\n",
        "      by dividing each element by 126. By default, True.\n",
        "    show_logs: bool\n",
        "      If True, shows the logs while the data is\n",
        "      being processed. By default, True.\n",
        "    num_data: int\n",
        "      Number of data points to be fetched from the \n",
        "      dataset. By default, 1000000.\n",
        "    to_one_hot: bool\n",
        "      If True, converts data points to one hot \n",
        "      notation using 0 for low and 1 for high.\n",
        "      By default, False.\n",
        "    cache_result: bool\n",
        "      If True, caches the pre-processed data to \n",
        "      cache files of `npy` format. By default, False.\n",
        "    use_cache: bool\n",
        "      If True, uses the previously cached data, if possible.\n",
        "      By default, False.\n",
        "    \n",
        "    Returns\n",
        "    =======\n",
        "\n",
        "    np.array\n",
        "\n",
        "    Note\n",
        "    ====\n",
        "\n",
        "    This function was specifically written for MNIST database\n",
        "    of handwritten digits.\n",
        "    \"\"\"\n",
        "    if use_cache:\n",
        "        filename = filename + \"_cache.npy\"\n",
        "        ret_val = np.load(filename)\n",
        "        print(\"Loaded cached data from %s\"%(filename))\n",
        "        return ret_val\n",
        "    with open(filename, 'rb') as f:\n",
        "        zero, data_type, dims = struct.unpack('>HBB', f.read(4))\n",
        "        shape = tuple(struct.unpack('>I', f.read(4))[0] for d in range(dims))\n",
        "        ret_val = np.fromstring(f.read(), dtype=np.uint8).reshape(shape)\n",
        "        num_data = min(ret_val.shape[0], num_data)\n",
        "        normalize_val = []\n",
        "        if normalize:\n",
        "            for i in range(num_data):\n",
        "                mat = [[0 for _i in range(ret_val.shape[1])]\n",
        "                        for _j in range(ret_val.shape[2])]\n",
        "                if show_logs:\n",
        "                    print(\"Normalized %s-th data\"%(i+1))\n",
        "                for j in range(ret_val.shape[1]):\n",
        "                    for k in range(ret_val.shape[2]):\n",
        "                        mat[j][k] = ret_val[i][j][k]/126.\n",
        "                normalize_val.append(mat)\n",
        "                del mat\n",
        "            ret_val = np.asarray(normalize_val, dtype=np.float32)\n",
        "        del normalize_val\n",
        "        flatten_val = []\n",
        "        if flatten:\n",
        "            for i in range(num_data):\n",
        "                if show_logs:\n",
        "                    print(\"Flattened %s-th data\"%(i+1))\n",
        "                flatten_val.append(ret_val[i].flatten('C'))\n",
        "            ret_val = np.asarray(flatten_val, dtype=np.float32)\n",
        "        del flatten_val\n",
        "        if to_one_hot:\n",
        "            ret_val = one_hot(ret_val[0:num_data], 10)\n",
        "        if cache_result:\n",
        "            np.save(filename+\"_cache\", ret_val)\n",
        "            print(\"Saved cached data to %s_cache\"%(filename))\n",
        "        return ret_val"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDg7ma_8rGs1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "class ANNLayer(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Base class for ANN layers.\n",
        "\n",
        "    Parameters\n",
        "    ==========\n",
        "\n",
        "    num_inputs: int\n",
        "        Number of inputs to the layer.\n",
        "    num_outputs: int\n",
        "        Number of outputs from the layer.\n",
        "    activation:\n",
        "        Activation from tensorflow.keras.activations.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_inputs, num_outputs, activation):\n",
        "        super(ANNLayer, self).__init__(dtype=tf.float32)\n",
        "        self.num_outputs = num_outputs\n",
        "        self.activation = activation\n",
        "        self.kernel_mu = self.add_variable(\"kernel_mu\",\n",
        "                                            shape=[num_inputs,\n",
        "                                                   self.num_outputs],\n",
        "                                            initializer=tf.keras.initializers.TruncatedNormal(),\n",
        "                                            dtype=tf.float32)\n",
        "\n",
        "    def call(self, input):\n",
        "        \"\"\"\n",
        "        Implements the feed forward operation when layer \n",
        "        is called on the given input.\n",
        "\n",
        "        input: tf.Tensor\n",
        "          The input.\n",
        "        weights: tf.Tensor\n",
        "          The weights\n",
        "        \n",
        "        Returns\n",
        "        =======\n",
        "\n",
        "        tf.Tensor\n",
        "        \"\"\"\n",
        "        prod = self.activation(tf.matmul(input, self.kernel_mu))\n",
        "        return prod\n",
        "\n",
        "class BNNLayer(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Base class for BNN layers.\n",
        "\n",
        "    Parameters\n",
        "    ==========\n",
        "\n",
        "    num_inputs: int\n",
        "        Number of inputs to the layer.\n",
        "    num_outputs: int\n",
        "        Number of outputs from the layer.\n",
        "    activation:\n",
        "        Activation from tensorflow.keras.activations.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_inputs, num_outputs, activation):\n",
        "        super(BNNLayer, self).__init__(dtype=tf.float32)\n",
        "        self.num_outputs = num_outputs\n",
        "        self.activation = activation\n",
        "        self.kernel_mu = self.add_variable(\"kernel_mu\",\n",
        "                                            shape=[num_inputs,\n",
        "                                                   self.num_outputs],\n",
        "                                            initializer=tf.keras.initializers.TruncatedNormal(),\n",
        "                                            dtype=tf.float32)\n",
        "        self.kernel_rho = self.add_variable(\"kernel_sigma\",\n",
        "                                            shape=[num_inputs,\n",
        "                                                   self.num_outputs],\n",
        "                                            initializer=tf.keras.initializers.TruncatedNormal(),\n",
        "                                            dtype=tf.float32)\n",
        "\n",
        "    def _reparametrize(self):\n",
        "        \"\"\"\n",
        "        Abstract method which implements the\n",
        "        reparametrisation technique.\n",
        "        \"\"\"\n",
        "        return None\n",
        "\n",
        "    def call(self, input, weights):\n",
        "        \"\"\"\n",
        "        Implements the inference operation when layer \n",
        "        is called on the given input and weights.\n",
        "\n",
        "        input: tf.Tensor\n",
        "          The input.\n",
        "        weights: tf.Tensor\n",
        "          The weights\n",
        "        \n",
        "        Returns\n",
        "        =======\n",
        "\n",
        "        tf.Tensor\n",
        "        \"\"\"\n",
        "        prod = self.activation(tf.matmul(input, weights))\n",
        "        return prod\n",
        "\n",
        "class BNNLayer_Normal_Normal(BNNLayer):\n",
        "    \"\"\"\n",
        "    BNN layer which implements reparametrisation\n",
        "    trick from N(0, 1) to any N(mu, sigma).\n",
        "    \"\"\"\n",
        "\n",
        "    def _reparametrize(self):\n",
        "        eps_w_shape = self.kernel_mu.shape\n",
        "        eps_w = tf.random.normal(eps_w_shape, 0, 0.01, dtype=tf.float32)\n",
        "        term_w = tf.math.multiply(eps_w,\n",
        "                                  tf.math.log(tf.math.add(\n",
        "                                  tf.math.exp(tf.clip_by_value(self.kernel_rho, -87.315, 88.722)),\n",
        "                                  tf.constant(1., shape=eps_w_shape, dtype=tf.float32))))\n",
        "        return tf.math.add(self.kernel_mu, term_w)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-24sIjZrRCg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.activations import relu as Relu, elu as Elu, softmax as Softmax\n",
        "import math\n",
        "\n",
        "class ANN(tf.keras.Model):\n",
        "    \"\"\"\n",
        "    Artificial Neural Network using point estimates\n",
        "    of underlying distribution of training data.\n",
        "\n",
        "    Parameters\n",
        "    ==========\n",
        "\n",
        "    input_shape: tuple\n",
        "      By default, None.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_shape=None):\n",
        "        super(ANN, self).__init__()\n",
        "        self.InputLayer = tf.keras.layers.InputLayer(input_shape=(input_shape[1],),\n",
        "                            batch_size=input_shape[0], dtype=tf.float32)\n",
        "        self.Dense_1 = ANNLayer(int(input_shape[-1]), 400, activation=Relu)\n",
        "        self.Dense_2 = ANNLayer(400, 400, activation=Relu)\n",
        "        self.Output = ANNLayer(400, 10, activation=Softmax)\n",
        "        self.Layers = [self.Dense_1, self.Dense_2, self.Output]\n",
        "\n",
        "    def run(self, inputs):\n",
        "        \"\"\"\n",
        "        Performs feed forward operation on the given inputs.\n",
        "\n",
        "        Parameters\n",
        "        ==========\n",
        "\n",
        "        inputs: tf.Tensor\n",
        "\n",
        "        Returns\n",
        "        =======\n",
        "\n",
        "        tf.Tensor\n",
        "        \"\"\"\n",
        "        layer_output = self.InputLayer(inputs)\n",
        "        i = 0\n",
        "        for layer in self.Layers:\n",
        "            layer_output = layer(layer_output)\n",
        "            i += 1\n",
        "        return layer_output\n",
        "\n",
        "    def get_loss(self, inputs, targets, inference=False):\n",
        "        \"\"\"\n",
        "        Computes the total training loss.\n",
        "\n",
        "        Parameters\n",
        "        ==========\n",
        "\n",
        "        inputs: tf.Tensor\n",
        "            Input to the layers.\n",
        "        targets: tf.Tensor\n",
        "            True targets that the model wants to learn from.\n",
        "        inference: bool\n",
        "            Used to determine the order of the outputs in the tuple\n",
        "            being returned.\n",
        "        \n",
        "        Returns\n",
        "        =======\n",
        "\n",
        "        tuple\n",
        "          Containing loss and output of neural network for each sample.\n",
        "        \"\"\"\n",
        "        outputs = self.run(inputs)\n",
        "        loss = tf.reduce_mean(tf.keras.losses.categorical_crossentropy(targets, outputs))\n",
        "\n",
        "        if inference:\n",
        "            return outputs, loss\n",
        "        return loss, outputs\n",
        "\n",
        "class BNN_Normal_Normal(tf.keras.Model):\n",
        "    \"\"\"\n",
        "    Neural Network which uses, `BNNLayer_Normal_Normal` layers.\n",
        "\n",
        "    Parameters\n",
        "    ==========\n",
        "\n",
        "    input_shape: tuple\n",
        "      By default, None.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_shape=None):\n",
        "        super(BNN_Normal_Normal, self).__init__()\n",
        "        self.InputLayer = tf.keras.layers.InputLayer(input_shape=(input_shape[1],),\n",
        "                            batch_size=input_shape[0], dtype=tf.float32)\n",
        "        self.Dense_1 = BNNLayer_Normal_Normal(int(input_shape[-1]), 400, activation=Relu)\n",
        "        self.Dense_2 = BNNLayer_Normal_Normal(400, 400, activation=Relu)\n",
        "        self.Output = BNNLayer_Normal_Normal(400, 10, activation=Softmax)\n",
        "        self.Layers = [self.Dense_1, self.Dense_2, self.Output]\n",
        "\n",
        "    def run(self, inputs, *weights):\n",
        "        \"\"\"\n",
        "        Produces neural network's outputs for\n",
        "        given inputs and weights.\n",
        "\n",
        "        Parameters\n",
        "        ==========\n",
        "\n",
        "        inputs: tf.Tensor/np.array\n",
        "        weights: tf.Tensor\n",
        "\n",
        "        Returns\n",
        "        =======\n",
        "\n",
        "        tf.Tensor\n",
        "        \"\"\"\n",
        "        layer_output = self.InputLayer(inputs)\n",
        "        i = 0\n",
        "        for layer in self.Layers:\n",
        "            layer_output = layer(layer_output, weights[i])\n",
        "            i += 1\n",
        "        return layer_output\n",
        "\n",
        "    def log_prior(self, weights):\n",
        "        \"\"\"\n",
        "        Computes the natural logarithm of scale\n",
        "        mixture prior of weights.\n",
        "\n",
        "        Parameters\n",
        "        ==========\n",
        "\n",
        "        weights: tf.Tensor\n",
        "\n",
        "        Returns\n",
        "        =======\n",
        "\n",
        "        tf.Tensor\n",
        "\n",
        "        Note\n",
        "        ====\n",
        "\n",
        "        The two standard deviations of the scale mixture are,\n",
        "        exp(0) and exp(-6). The weight of both normal distributions\n",
        "        is 0.5.\n",
        "        \"\"\"\n",
        "        shape = weights.shape\n",
        "        sigma_1 = tf.constant(math.exp(0), shape=shape, dtype=tf.float32)\n",
        "        sigma_2 = tf.constant(math.exp(-6), shape=shape, dtype=tf.float32)\n",
        "        def pdf(w, sigma):\n",
        "            res1 = tf.math.divide(tf.math.square(w), tf.math.square(sigma)*2)\n",
        "            return tf.math.divide(tf.math.exp(tf.clip_by_value(-res1, -87.315, 88.722)), sigma*(2*math.pi)**0.5)\n",
        "        part_1 = tf.clip_by_value(0.25*pdf(weights, sigma_1), tf.float32.min//2, tf.float32.max//2)\n",
        "        part_2 = tf.clip_by_value(0.75*pdf(weights, sigma_2), tf.float32.min//2, tf.float32.max//2)\n",
        "        return tf.math.reduce_sum(tf.math.log(part_1 + part_2))\n",
        "\n",
        "    def log_posterior(self, weights, mu, rho):\n",
        "        \"\"\"\n",
        "        Computes the natural logarithm of Gaussian\n",
        "        posterior on weights.\n",
        "\n",
        "        Parameters\n",
        "        ==========\n",
        "\n",
        "        weights: tf.Tensor\n",
        "        mu: tf.Tensor\n",
        "          The mean of the posterior Gaussian distribution.\n",
        "        rho: tf.Tensor\n",
        "          Used to compute the variance of the posterior Gaussian distribution.\n",
        "        \n",
        "        Returns\n",
        "        =======\n",
        "\n",
        "        tf.Tensor\n",
        "        \"\"\"\n",
        "        def pdf(w, mu, sigma):\n",
        "            res1 = tf.math.divide(tf.math.square(w - mu), tf.math.square(sigma)*2)\n",
        "            return tf.math.divide(tf.math.exp(tf.clip_by_value(-res1, -87.315, 88.722)), sigma*(2*math.pi)**0.5)\n",
        "        sigma = tf.math.log(tf.math.add(\n",
        "                                  tf.math.exp(tf.clip_by_value(rho, -87.315, 88.722)),\n",
        "                                  tf.constant(1., shape=rho.shape, dtype=tf.float32)))\n",
        "        log_q = tf.math.log(tf.clip_by_value(pdf(weights, mu, sigma), tf.float32.min//2, tf.float32.max//2))\n",
        "        return tf.math.reduce_sum(log_q)\n",
        "\n",
        "    def get_loss(self, inputs, targets, samples, weight=1., inference=False):\n",
        "        \"\"\"\n",
        "        Computes the total training loss.\n",
        "\n",
        "        Parameters\n",
        "        ==========\n",
        "\n",
        "        inputs: tf.Tensor/np.array\n",
        "            Input to the layers.\n",
        "        targets: tf.Tensor/np.array\n",
        "            True targets that the model wants to learn from.\n",
        "        samples: int\n",
        "            The number of samples to be drawn for weights.\n",
        "        weight: tf.float32 or equivalent.\n",
        "            Weight given to loss of each batch. By default, 1.\n",
        "        inference: bool\n",
        "            Used to determine the order of the outputs in the tuple\n",
        "            being returned.\n",
        "        \n",
        "        Returns\n",
        "        =======\n",
        "\n",
        "        tuple\n",
        "          Containing loss and output of neural network for each sample.\n",
        "        \"\"\"\n",
        "        loss = tf.constant(0., dtype=tf.float32)\n",
        "        outputs_list = []\n",
        "        for _ in range(samples):\n",
        "            weights = []\n",
        "            pw, qw = tf.constant(0, dtype=tf.float32), tf.constant(0, dtype=tf.float32)\n",
        "            for layer in self.Layers:\n",
        "                weights.append(layer._reparametrize())\n",
        "                kernel_mu, kernel_rho = layer.kernel_mu, layer.kernel_rho\n",
        "                pw += self.log_prior(weights[-1])\n",
        "                qw += self.log_posterior(weights[-1], kernel_mu, kernel_rho)\n",
        "\n",
        "            outputs = self.run(inputs, *weights)\n",
        "            outputs_list.append(outputs)\n",
        "            cse = tf.reduce_mean(tf.keras.losses.categorical_crossentropy(targets, outputs))\n",
        "            if inference:\n",
        "                loss += cse\n",
        "            else:\n",
        "                loss += (qw - pw)*weight + tf.cast(cse, tf.float32)\n",
        "\n",
        "        if inference:\n",
        "            return outputs_list, loss/samples\n",
        "        return loss/samples, outputs_list\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-DNoWD4bVUl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_adversarial_pattern(model, input, target):\n",
        "    \"\"\"\n",
        "    Returns the sign of the gradient for the given model, inputs and targets\n",
        "    using Fast Gradient Sign Method.\n",
        "    \"\"\"\n",
        "    with tf.GradientTape() as tape:\n",
        "      tape.watch(input)\n",
        "      _, loss = model.get_loss(input, target, True)\n",
        "\n",
        "    gradient = tape.gradient(loss, input)\n",
        "    return tf.sign(gradient)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9pVNPQcrZ4G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from datetime import datetime\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "samples = 10\n",
        "test_points = 10000\n",
        "prefix = \"./drive/My Drive/bnn/\"\n",
        "\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import random, copy, math\n",
        "\n",
        "indices = [random.randint(0, test_points - 1) for i in range(5)]\n",
        "\n",
        "def display_image(image):\n",
        "    \"\"\"\n",
        "    Displayes image.\n",
        "\n",
        "    Parameters\n",
        "    ==========\n",
        "\n",
        "    image: np.array\n",
        "      Flattened image of size 784.\n",
        "\n",
        "    Note\n",
        "    ====\n",
        "\n",
        "    Specifically written for MNIST sized images.\n",
        "    \"\"\"\n",
        "    fig = plt.figure()\n",
        "    fig.set_size_inches((1, 1))\n",
        "    ax = plt.Axes(fig, [0., 0., 1., 1.])\n",
        "    ax.set_axis_off()\n",
        "    fig.add_axes(ax)\n",
        "    plt.set_cmap('gray')\n",
        "    new_image = []\n",
        "    for i in range(28):\n",
        "      row = []\n",
        "      for j in range(28):\n",
        "        row.append(math.ceil(image[i*28 + j]*126))\n",
        "      new_image.append(row)\n",
        "    ax.imshow(np.asarray(new_image))\n",
        "    plt.show()\n",
        "\n",
        "def shannon_entropy(preds, samples):\n",
        "    \"\"\"\n",
        "    Computes Shannon entropy.\n",
        "\n",
        "    Parameters\n",
        "    ==========\n",
        "\n",
        "    preds: tf.Tensor\n",
        "      Predictions made by the neural network.\n",
        "    samples: int\n",
        "      Number of times inferencing was done.\n",
        "    \n",
        "    Returns\n",
        "    =======\n",
        "\n",
        "    tf.float32\n",
        "    \"\"\"\n",
        "    total_entropy = 0\n",
        "    for i in range(test_points):\n",
        "      mean_prediction = 0\n",
        "      for j in range(samples):\n",
        "        mean_prediction += preds[j][i]/samples\n",
        "      total_entropy += -tf.reduce_mean(tf.math.multiply(mean_prediction, tf.math.log(mean_prediction)))\n",
        "    return total_entropy/test_points\n",
        "\n",
        "def test_adversarial_ann_mnist(file_path, input_shape, input_data, output_data, eps, cache_result, use_cache):\n",
        "    \"\"\"\n",
        "    Used for evaluating the trained `ANN` on adversarial versions of MNIST images.\n",
        "\n",
        "    Parameters\n",
        "    ==========\n",
        "\n",
        "    file_path: str\n",
        "      The path to the file from where the saved `BNN_Normal_Normal` is to be loaded.\n",
        "    input_shape: tuple\n",
        "    input_data: str\n",
        "      The path to the file containing input data. Must be binary.\n",
        "    output_data: str\n",
        "      The path to the file containing output_data. Must be binary.\n",
        "    eps: tf.float32\n",
        "      The amount of preturbations to be added to the image for Adversarial\n",
        "      attack using FGSM.\n",
        "    cache_result: bool\n",
        "      If True, caches the pre-processed data to \n",
        "      cache files of `npy` format. By default, True.\n",
        "    use_cache: bool\n",
        "      If True, uses the previously cached data, if possible.\n",
        "      By default, False.\n",
        "    \n",
        "    Returns\n",
        "    =======\n",
        "\n",
        "    Tuple of the following,\n",
        "\n",
        "    preds: list containing tf.Tensor\n",
        "      Predictions made by neural network.\n",
        "    targets: tf.Tensor\n",
        "      The target labels.\n",
        "    total_loss: tf.float32\n",
        "    \"\"\"\n",
        "    model = ANN(input_shape=input_shape)\n",
        "    model.load_weights(file_path)\n",
        "    inputs = read_idx(input_data, True, True, True, 10000, False, cache_result, use_cache)\n",
        "    targets = read_idx(output_data, False, False, True, 10000, True, cache_result, use_cache)\n",
        "    preds, total_loss = [], []\n",
        "    delta = create_adversarial_pattern(model, tf.convert_to_tensor(inputs), tf.convert_to_tensor(targets))\n",
        "    # if i in indices:\n",
        "    #     print(\"Original Image\")\n",
        "    #     display_image(input[0])\n",
        "    inputs = inputs + eps*delta\n",
        "    # if i in indices:\n",
        "    #     print(\"After Adding Preturbations\")\n",
        "    #     display_image(input[0])\n",
        "    preds, total_loss = model.get_loss(inputs, targets, True)\n",
        "    return preds, targets, total_loss\n",
        "\n",
        "def test_adversarial_bnn_mnist(file_path, input_shape, input_data, output_data, eps, cache_result, use_cache):\n",
        "    \"\"\"\n",
        "    Used for evaluating the trained `ANN` on adversarial versions of MNIST images.\n",
        "\n",
        "    Parameters\n",
        "    ==========\n",
        "\n",
        "    file_path: str\n",
        "      The path to the file from where the saved `BNN_Normal_Normal` is to be loaded.\n",
        "    input_shape: tuple\n",
        "    input_data: str\n",
        "      The path to the file containing input data. Must be binary.\n",
        "    output_data: str\n",
        "      The path to the file containing output_data. Must be binary.\n",
        "    eps: tf.float32\n",
        "      The amount of preturbations to be added to the image for Adversarial\n",
        "      attack using FGSM.\n",
        "    cache_result: bool\n",
        "      If True, caches the pre-processed data to \n",
        "      cache files of `npy` format. By default, True.\n",
        "    use_cache: bool\n",
        "      If True, uses the previously cached data, if possible.\n",
        "      By default, False.\n",
        "    \n",
        "    Returns\n",
        "    =======\n",
        "\n",
        "    Tuple of the following,\n",
        "\n",
        "    preds: list containing tf.Tensor\n",
        "      Predictions made by neural network.\n",
        "    targets: tf.Tensor\n",
        "      The target labels.\n",
        "    total_loss: tf.float32\n",
        "    uncertainty: tf.float32\n",
        "    \"\"\"\n",
        "    model = ANN(input_shape=input_shape)\n",
        "    model.load_weights(file_path)\n",
        "    modelBNN = BNN_Normal_Normal(input_shape=input_shape)\n",
        "    modelBNN.load_weights(file_path)\n",
        "    inputs = read_idx(input_data, True, True, True, 10000, False, cache_result, use_cache)\n",
        "    targets = read_idx(output_data, False, False, True, 10000, True, cache_result, use_cache)\n",
        "    preds, total_loss = [], []\n",
        "    delta = create_adversarial_pattern(model, tf.convert_to_tensor(inputs), tf.convert_to_tensor(targets))\n",
        "    # if i in indices:\n",
        "    #     print(\"Original Image\")\n",
        "    #     display_image(input[0])\n",
        "    inputs = inputs + eps*delta\n",
        "    # if i in indices:\n",
        "    #     print(\"After Adding Preturbations\")\n",
        "    #     display_image(input[0])\n",
        "    preds, total_loss = modelBNN.get_loss(inputs, targets, samples, 1., True)\n",
        "    uncertainty = shannon_entropy(preds, samples)\n",
        "    return preds, targets, total_loss, uncertainty\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZ9TJV1reYCb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "outputId": "cb42b02b-1928-4b96-c0db-a385f12a9adc"
      },
      "source": [
        "preds, targets, _ = test_adversarial_ann_mnist(prefix + \"weights/mnist/_2020_08_02_06_33_12_410605\", (None, 784), \n",
        "                                               prefix + \"datasets/mnist_test_images\", prefix + \"datasets/mnist_test_labels\",\n",
        "                                                0.071875, False, True)\n",
        "acc = 0\n",
        "for i in range(test_points):\n",
        "    if tf.math.argmax(preds[i]) == tf.math.argmax(targets[i]):\n",
        "        acc += 1\n",
        "print(\"Accuracy: \", acc/(test_points/100))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-3-31551de7919f>:26: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.add_weight` method instead.\n",
            "Loaded cached data from ./drive/My Drive/bnn/datasets/mnist_test_images_cache.npy\n",
            "Loaded cached data from ./drive/My Drive/bnn/datasets/mnist_test_labels_cache.npy\n",
            "Accuracy:  43.16\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BeddCZstqcSh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "outputId": "2611533f-3d25-44de-f164-e9b9a51d749d"
      },
      "source": [
        "preds, targets, _, uncertainty = test_adversarial_bnn_mnist(prefix + \"weights/mnist/2020_08_09_08_27_21_674420\", (None, 784), \n",
        "                                                            prefix + \"datasets/mnist_test_images\", prefix + \"datasets/mnist_test_labels\",\n",
        "                                                            0.071875, False, True)\n",
        "acc = 0\n",
        "oods = 0\n",
        "for i in range(test_points):\n",
        "  is_ood = False\n",
        "  curr_output = tf.math.argmax(preds[0][i])\n",
        "  for j in range(1, samples):\n",
        "    if curr_output != tf.math.argmax(preds[j][i]):\n",
        "      is_ood = True\n",
        "      break\n",
        "  if is_ood:\n",
        "    oods += 1\n",
        "  else:\n",
        "      if curr_output == tf.math.argmax(targets[i]):\n",
        "        acc += 1\n",
        "\n",
        "print(\"Accuracy: \", acc/(test_points/100))\n",
        "print(\"OOD Inputs: \", oods)\n",
        "print(\"Uncertainty: \", uncertainty)\n",
        "\n",
        "acc = 0\n",
        "for i in range(test_points):\n",
        "  for j in range(samples):\n",
        "    if tf.math.argmax(preds[j][i]) == tf.math.argmax(targets[i]):\n",
        "        acc += 1\n",
        "print(\"Accuracy: \", acc/((test_points*samples)/100))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded cached data from ./drive/My Drive/bnn/datasets/mnist_test_images_cache.npy\n",
            "Loaded cached data from ./drive/My Drive/bnn/datasets/mnist_test_labels_cache.npy\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).Dense_1.kernel_sigma\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).Dense_2.kernel_sigma\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).Output.kernel_sigma\n",
            "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
            "Accuracy:  25.42\n",
            "OOD Inputs:  4281\n",
            "Uncertainty:  tf.Tensor(0.05322278, shape=(), dtype=float32)\n",
            "Accuracy:  44.141\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}