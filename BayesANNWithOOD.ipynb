{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BayesANNWithOOD.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHJgv3oM6CbE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "cf4eb0ea-4b06-44ba-ceff-a9759be1e052"
      },
      "source": [
        "\"\"\"\n",
        "Mounts your drive for reading and writing datasets, results, weights.\n",
        "\"\"\"\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSXLZt_GqW19",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import struct\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "def one_hot(a, num_classes):\n",
        "  \"\"\"\n",
        "  Converts the input numpy array to one hot.\n",
        "\n",
        "  Parameters\n",
        "  ==========\n",
        "\n",
        "  a: numpy.array\n",
        "    The input array.\n",
        "  num_classes: int\n",
        "    The number of classes to be used for one hot notation.\n",
        "  \n",
        "  Returns\n",
        "  =======\n",
        "\n",
        "  np.array\n",
        "  \"\"\"\n",
        "  return np.squeeze(np.eye(num_classes, dtype=np.float32)[a.reshape(-1)])\n",
        "\n",
        "def read_idx(filename, flatten=True, normalize=True, show_logs=True,\n",
        "             num_data=1000000, to_one_hot=False,\n",
        "             cache_result=False, use_cache=False):\n",
        "    \"\"\"\n",
        "    Reads datasets from binary files.\n",
        "\n",
        "    filename: str\n",
        "      The path to the file where the dataset is stored.\n",
        "      Make sure to not add any extensions at the end of\n",
        "      the filepath.\n",
        "    flatten: bool\n",
        "      If True, flattens the data points. By default, True.\n",
        "    normalize: bool\n",
        "      If True, the data points are normalised\n",
        "      by dividing each element by 126. By default, True.\n",
        "    show_logs: bool\n",
        "      If True, shows the logs while the data is\n",
        "      being processed. By default, True.\n",
        "    num_data: int\n",
        "      Number of data points to be fetched from the \n",
        "      dataset. By default, 1000000.\n",
        "    to_one_hot: bool\n",
        "      If True, converts data points to one hot \n",
        "      notation using 0 for low and 1 for high.\n",
        "      By default, False.\n",
        "    cache_result: bool\n",
        "      If True, caches the pre-processed data to \n",
        "      cache files of `npy` format. By default, False.\n",
        "    use_cache: bool\n",
        "      If True, uses the previously cached data, if possible.\n",
        "      By default, False.\n",
        "    \n",
        "    Returns\n",
        "    =======\n",
        "\n",
        "    np.array\n",
        "\n",
        "    Note\n",
        "    ====\n",
        "\n",
        "    This function was specifically written for MNIST database\n",
        "    of handwritten digits.\n",
        "    \"\"\"\n",
        "    if use_cache:\n",
        "        filename = filename + \"_cache.npy\"\n",
        "        ret_val = np.load(filename)\n",
        "        print(\"Loaded cached data from %s\"%(filename))\n",
        "        return ret_val\n",
        "    with open(filename, 'rb') as f:\n",
        "        zero, data_type, dims = struct.unpack('>HBB', f.read(4))\n",
        "        shape = tuple(struct.unpack('>I', f.read(4))[0] for d in range(dims))\n",
        "        ret_val = np.fromstring(f.read(), dtype=np.uint8).reshape(shape)\n",
        "        num_data = min(ret_val.shape[0], num_data)\n",
        "        normalize_val = []\n",
        "        if normalize:\n",
        "            for i in range(num_data):\n",
        "                mat = [[0 for _i in range(ret_val.shape[1])]\n",
        "                        for _j in range(ret_val.shape[2])]\n",
        "                if show_logs:\n",
        "                    print(\"Normalized %s-th data\"%(i+1))\n",
        "                for j in range(ret_val.shape[1]):\n",
        "                    for k in range(ret_val.shape[2]):\n",
        "                        mat[j][k] = ret_val[i][j][k]/126.\n",
        "                normalize_val.append(mat)\n",
        "                del mat\n",
        "            ret_val = np.asarray(normalize_val, dtype=np.float32)\n",
        "        del normalize_val\n",
        "        flatten_val = []\n",
        "        if flatten:\n",
        "            for i in range(num_data):\n",
        "                if show_logs:\n",
        "                    print(\"Flattened %s-th data\"%(i+1))\n",
        "                flatten_val.append(ret_val[i].flatten('C'))\n",
        "            ret_val = np.asarray(flatten_val, dtype=np.float32)\n",
        "        del flatten_val\n",
        "        if to_one_hot:\n",
        "            ret_val = one_hot(ret_val[0:num_data], 10)\n",
        "        if cache_result:\n",
        "            np.save(filename+\"_cache\", ret_val)\n",
        "            print(\"Saved cached data to %s_cache\"%(filename))\n",
        "        return ret_val"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDg7ma_8rGs1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "class ANNLayer(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Base class for ANN layers.\n",
        "\n",
        "    Parameters\n",
        "    ==========\n",
        "\n",
        "    num_inputs: int\n",
        "        Number of inputs to the layer.\n",
        "    num_outputs: int\n",
        "        Number of outputs from the layer.\n",
        "    activation:\n",
        "        Activation from tensorflow.keras.activations.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_inputs, num_outputs, activation):\n",
        "        super(ANNLayer, self).__init__(dtype=tf.float32)\n",
        "        self.num_outputs = num_outputs\n",
        "        self.activation = activation\n",
        "        self.kernel_mu = self.add_variable(\"kernel_mu\",\n",
        "                                            shape=[num_inputs,\n",
        "                                                   self.num_outputs],\n",
        "                                            initializer=tf.keras.initializers.TruncatedNormal(),\n",
        "                                            dtype=tf.float32)\n",
        "\n",
        "    def call(self, input):\n",
        "        \"\"\"\n",
        "        Implements the feed forward operation when layer \n",
        "        is called on the given input.\n",
        "\n",
        "        input: tf.Tensor\n",
        "          The input.\n",
        "        weights: tf.Tensor\n",
        "          The weights\n",
        "        \n",
        "        Returns\n",
        "        =======\n",
        "\n",
        "        tf.Tensor\n",
        "        \"\"\"\n",
        "        prod = self.activation(tf.matmul(input, self.kernel_mu))\n",
        "        return prod\n",
        "\n",
        "class BNNLayer(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Base class for BNN layers.\n",
        "\n",
        "    Parameters\n",
        "    ==========\n",
        "\n",
        "    num_inputs: int\n",
        "        Number of inputs to the layer.\n",
        "    num_outputs: int\n",
        "        Number of outputs from the layer.\n",
        "    activation:\n",
        "        Activation from tensorflow.keras.activations.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_inputs, num_outputs, activation):\n",
        "        super(BNNLayer, self).__init__(dtype=tf.float32)\n",
        "        self.num_outputs = num_outputs\n",
        "        self.activation = activation\n",
        "        self.kernel_mu = self.add_variable(\"kernel_mu\",\n",
        "                                            shape=[num_inputs,\n",
        "                                                   self.num_outputs],\n",
        "                                            initializer=tf.keras.initializers.TruncatedNormal(),\n",
        "                                            dtype=tf.float32)\n",
        "        self.kernel_rho = self.add_variable(\"kernel_sigma\",\n",
        "                                            shape=[num_inputs,\n",
        "                                                   self.num_outputs],\n",
        "                                            initializer=tf.keras.initializers.TruncatedNormal(),\n",
        "                                            dtype=tf.float32)\n",
        "\n",
        "    def _reparametrize(self):\n",
        "        \"\"\"\n",
        "        Abstract method which implements the\n",
        "        reparametrisation technique.\n",
        "        \"\"\"\n",
        "        return None\n",
        "\n",
        "    def call(self, input, weights):\n",
        "        \"\"\"\n",
        "        Implements the inference operation when layer \n",
        "        is called on the given input and weights.\n",
        "\n",
        "        input: tf.Tensor\n",
        "          The input.\n",
        "        weights: tf.Tensor\n",
        "          The weights\n",
        "        \n",
        "        Returns\n",
        "        =======\n",
        "\n",
        "        tf.Tensor\n",
        "        \"\"\"\n",
        "        prod = self.activation(tf.matmul(input, weights))\n",
        "        return prod\n",
        "\n",
        "class BNNLayer_Normal_Normal(BNNLayer):\n",
        "    \"\"\"\n",
        "    BNN layer which implements reparametrisation\n",
        "    trick from N(0, 1) to any N(mu, sigma).\n",
        "    \"\"\"\n",
        "\n",
        "    def _reparametrize(self):\n",
        "        eps_w_shape = self.kernel_mu.shape\n",
        "        eps_w = tf.random.normal(eps_w_shape, 0, 0.01, dtype=tf.float32)\n",
        "        term_w = tf.math.multiply(eps_w,\n",
        "                                  tf.math.log(tf.math.add(\n",
        "                                  tf.math.exp(tf.clip_by_value(self.kernel_rho, -87.315, 88.722)),\n",
        "                                  tf.constant(1., shape=eps_w_shape, dtype=tf.float32))))\n",
        "        return tf.math.add(self.kernel_mu, term_w)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-24sIjZrRCg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.activations import relu as Relu, elu as Elu, softmax as Softmax\n",
        "import math\n",
        "\n",
        "class ANN(tf.keras.Model):\n",
        "    \"\"\"\n",
        "    Artificial Neural Network using point estimates\n",
        "    of underlying distribution of training data.\n",
        "\n",
        "    Parameters\n",
        "    ==========\n",
        "\n",
        "    input_shape: tuple\n",
        "      By default, None.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_shape=None):\n",
        "        super(ANN, self).__init__()\n",
        "        self.InputLayer = tf.keras.layers.InputLayer(input_shape=(input_shape[1],),\n",
        "                            batch_size=input_shape[0], dtype=tf.float32)\n",
        "        self.Dense_1 = ANNLayer(int(input_shape[-1]), 400, activation=Relu)\n",
        "        self.Dense_2 = ANNLayer(400, 400, activation=Relu)\n",
        "        self.Output = ANNLayer(400, 10, activation=Softmax)\n",
        "        self.Layers = [self.Dense_1, self.Dense_2, self.Output]\n",
        "\n",
        "    def run(self, inputs):\n",
        "        \"\"\"\n",
        "        Performs feed forward operation on the given inputs.\n",
        "\n",
        "        Parameters\n",
        "        ==========\n",
        "\n",
        "        inputs: tf.Tensor\n",
        "\n",
        "        Returns\n",
        "        =======\n",
        "\n",
        "        tf.Tensor\n",
        "        \"\"\"\n",
        "        layer_output = self.InputLayer(inputs)\n",
        "        i = 0\n",
        "        for layer in self.Layers:\n",
        "            layer_output = layer(layer_output)\n",
        "            i += 1\n",
        "        return layer_output\n",
        "\n",
        "    def get_loss(self, inputs, targets, inference=False):\n",
        "        \"\"\"\n",
        "        Computes the total training loss.\n",
        "\n",
        "        Parameters\n",
        "        ==========\n",
        "\n",
        "        inputs: tf.Tensor\n",
        "            Input to the layers.\n",
        "        targets: tf.Tensor\n",
        "            True targets that the model wants to learn from.\n",
        "        inference: bool\n",
        "            Used to determine the order of the outputs in the tuple\n",
        "            being returned.\n",
        "        \n",
        "        Returns\n",
        "        =======\n",
        "\n",
        "        tuple\n",
        "          Containing loss and output of neural network for each sample.\n",
        "        \"\"\"\n",
        "        outputs = self.run(inputs)\n",
        "        loss = tf.reduce_mean(tf.keras.losses.categorical_crossentropy(targets, outputs))\n",
        "\n",
        "        if inference:\n",
        "            return outputs, loss\n",
        "        return loss, outputs\n",
        "\n",
        "class BNN_Normal_Normal(tf.keras.Model):\n",
        "    \"\"\"\n",
        "    Neural Network which uses, `BNNLayer_Normal_Normal` layers.\n",
        "\n",
        "    Parameters\n",
        "    ==========\n",
        "\n",
        "    input_shape: tuple\n",
        "      By default, None.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_shape=None):\n",
        "        super(BNN_Normal_Normal, self).__init__()\n",
        "        self.InputLayer = tf.keras.layers.InputLayer(input_shape=(input_shape[1],),\n",
        "                            batch_size=input_shape[0], dtype=tf.float32)\n",
        "        self.Dense_1 = BNNLayer_Normal_Normal(int(input_shape[-1]), 400, activation=Relu)\n",
        "        self.Dense_2 = BNNLayer_Normal_Normal(400, 400, activation=Relu)\n",
        "        self.Output = BNNLayer_Normal_Normal(400, 10, activation=Softmax)\n",
        "        self.Layers = [self.Dense_1, self.Dense_2, self.Output]\n",
        "\n",
        "    def run(self, inputs, *weights):\n",
        "        \"\"\"\n",
        "        Produces neural network's outputs for\n",
        "        given inputs and weights.\n",
        "\n",
        "        Parameters\n",
        "        ==========\n",
        "\n",
        "        inputs: tf.Tensor/np.array\n",
        "        weights: tf.Tensor\n",
        "\n",
        "        Returns\n",
        "        =======\n",
        "\n",
        "        tf.Tensor\n",
        "        \"\"\"\n",
        "        layer_output = self.InputLayer(inputs)\n",
        "        i = 0\n",
        "        for layer in self.Layers:\n",
        "            layer_output = layer(layer_output, weights[i])\n",
        "            i += 1\n",
        "        return layer_output\n",
        "\n",
        "    def log_prior(self, weights):\n",
        "        \"\"\"\n",
        "        Computes the natural logarithm of scale\n",
        "        mixture prior of weights.\n",
        "\n",
        "        Parameters\n",
        "        ==========\n",
        "\n",
        "        weights: tf.Tensor\n",
        "\n",
        "        Returns\n",
        "        =======\n",
        "\n",
        "        tf.Tensor\n",
        "\n",
        "        Note\n",
        "        ====\n",
        "\n",
        "        The two standard deviations of the scale mixture are,\n",
        "        exp(0) and exp(-6). The weight of both normal distributions\n",
        "        is 0.5.\n",
        "        \"\"\"\n",
        "        shape = weights.shape\n",
        "        sigma_1 = tf.constant(math.exp(0), shape=shape, dtype=tf.float32)\n",
        "        sigma_2 = tf.constant(math.exp(-6), shape=shape, dtype=tf.float32)\n",
        "        def pdf(w, sigma):\n",
        "            res1 = tf.math.divide(tf.math.square(w), tf.math.square(sigma)*2)\n",
        "            return tf.math.divide(tf.math.exp(tf.clip_by_value(-res1, -87.315, 88.722)), sigma*(2*math.pi)**0.5)\n",
        "        part_1 = tf.clip_by_value(0.25*pdf(weights, sigma_1), tf.float32.min//2, tf.float32.max//2)\n",
        "        part_2 = tf.clip_by_value(0.75*pdf(weights, sigma_2), tf.float32.min//2, tf.float32.max//2)\n",
        "        return tf.math.reduce_sum(tf.math.log(part_1 + part_2))\n",
        "\n",
        "    def log_posterior(self, weights, mu, rho):\n",
        "        \"\"\"\n",
        "        Computes the natural logarithm of Gaussian\n",
        "        posterior on weights.\n",
        "\n",
        "        Parameters\n",
        "        ==========\n",
        "\n",
        "        weights: tf.Tensor\n",
        "        mu: tf.Tensor\n",
        "          The mean of the posterior Gaussian distribution.\n",
        "        rho: tf.Tensor\n",
        "          Used to compute the variance of the posterior Gaussian distribution.\n",
        "        \n",
        "        Returns\n",
        "        =======\n",
        "\n",
        "        tf.Tensor\n",
        "        \"\"\"\n",
        "        def pdf(w, mu, sigma):\n",
        "            res1 = tf.math.divide(tf.math.square(w - mu), tf.math.square(sigma)*2)\n",
        "            return tf.math.divide(tf.math.exp(tf.clip_by_value(-res1, -87.315, 88.722)), sigma*(2*math.pi)**0.5)\n",
        "        sigma = tf.math.log(tf.math.add(\n",
        "                                  tf.math.exp(tf.clip_by_value(rho, -87.315, 88.722)),\n",
        "                                  tf.constant(1., shape=rho.shape, dtype=tf.float32)))\n",
        "        log_q = tf.math.log(tf.clip_by_value(pdf(weights, mu, sigma), tf.float32.min//2, tf.float32.max//2))\n",
        "        return tf.math.reduce_sum(log_q)\n",
        "\n",
        "    def get_loss(self, inputs, targets, samples, weight=1., inference=False):\n",
        "        \"\"\"\n",
        "        Computes the total training loss.\n",
        "\n",
        "        Parameters\n",
        "        ==========\n",
        "\n",
        "        inputs: tf.Tensor/np.array\n",
        "            Input to the layers.\n",
        "        targets: tf.Tensor/np.array\n",
        "            True targets that the model wants to learn from.\n",
        "        samples: int\n",
        "            The number of samples to be drawn for weights.\n",
        "        weight: tf.float32 or equivalent.\n",
        "            Weight given to loss of each batch. By default, 1.\n",
        "        inference: bool\n",
        "            Used to determine the order of the outputs in the tuple\n",
        "            being returned.\n",
        "        \n",
        "        Returns\n",
        "        =======\n",
        "\n",
        "        tuple\n",
        "          Containing loss and output of neural network for each sample.\n",
        "        \"\"\"\n",
        "        loss = tf.constant(0., dtype=tf.float32)\n",
        "        outputs_list = []\n",
        "        for _ in range(samples):\n",
        "            weights = []\n",
        "            pw, qw = tf.constant(0, dtype=tf.float32), tf.constant(0, dtype=tf.float32)\n",
        "            for layer in self.Layers:\n",
        "                weights.append(layer._reparametrize())\n",
        "                kernel_mu, kernel_rho = layer.kernel_mu, layer.kernel_rho\n",
        "                pw += self.log_prior(weights[-1])\n",
        "                qw += self.log_posterior(weights[-1], kernel_mu, kernel_rho)\n",
        "\n",
        "            outputs = self.run(inputs, *weights)\n",
        "            outputs_list.append(outputs)\n",
        "            cse = tf.reduce_mean(tf.keras.losses.categorical_crossentropy(targets, outputs))\n",
        "            if inference:\n",
        "                loss += cse\n",
        "            else:\n",
        "                loss += (qw - pw)*weight + tf.cast(cse, tf.float32)\n",
        "\n",
        "        if inference:\n",
        "            return outputs_list, loss/samples\n",
        "        return loss/samples, outputs_list"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9pVNPQcrZ4G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from datetime import datetime\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "samples = 10\n",
        "prefix = \"./drive/My Drive/bnn/\"\n",
        "\n",
        "def shannon_entropy(preds, samples):\n",
        "    \"\"\"\n",
        "    Computes Shannon entropy.\n",
        "\n",
        "    Parameters\n",
        "    ==========\n",
        "\n",
        "    preds: tf.Tensor\n",
        "      Predictions made by the neural network.\n",
        "    samples: int\n",
        "      Number of times inferencing was done.\n",
        "    \n",
        "    Returns\n",
        "    =======\n",
        "\n",
        "    tf.float32\n",
        "    \"\"\"\n",
        "    total_entropy = 0\n",
        "    for i in range(10000):\n",
        "      mean_prediction = 0\n",
        "      for j in range(samples):\n",
        "        mean_prediction += preds[j][i]/samples\n",
        "      total_entropy += -tf.reduce_mean(tf.math.multiply(mean_prediction, tf.math.log(mean_prediction)))\n",
        "    return total_entropy/10000\n",
        "\n",
        "def test_mnist(file_path, input_shape, input_data, output_data, cache_result, use_cache):\n",
        "    \"\"\"\n",
        "    Used for evaluating the trained `ANN` on test split of MNIST images.\n",
        "\n",
        "    Parameters\n",
        "    ==========\n",
        "\n",
        "    file_path: str\n",
        "      The path to the file from where the saved `BNN_Normal_Normal` is to be loaded.\n",
        "    input_shape: tuple\n",
        "    input_data: str\n",
        "      The path to the file containing input data. Must be binary.\n",
        "    output_data: str\n",
        "      The path to the file containing output_data. Must be binary.\n",
        "    cache_result: bool\n",
        "      If True, caches the pre-processed data to \n",
        "      cache files of `npy` format. By default, True.\n",
        "    use_cache: bool\n",
        "      If True, uses the previously cached data, if possible.\n",
        "      By default, False.\n",
        "    \n",
        "    Returns\n",
        "    =======\n",
        "\n",
        "    Tuple of the following,\n",
        "\n",
        "    preds: list containing tf.Tensor\n",
        "      Predictions made by neural network.\n",
        "    targets: tf.Tensor\n",
        "      The target labels.\n",
        "    total_loss: tf.float32\n",
        "    uncertainty: tf.float32\n",
        "    \"\"\"\n",
        "    model = BNN_Normal_Normal(input_shape=input_shape)\n",
        "    model.load_weights(file_path)\n",
        "    inputs = read_idx(input_data, True, True, True, 10000, False, cache_result, use_cache)\n",
        "    targets = read_idx(output_data, False, False, True, 10000, True, cache_result, use_cache)\n",
        "    preds, total_loss = model.get_loss(inputs, targets, samples, 1., True)\n",
        "    uncertainty = shannon_entropy(preds, samples)\n",
        "    return preds, targets, total_loss, uncertainty\n",
        "\n",
        "def test_fmnist(file_path, input_shape):\n",
        "    \"\"\"\n",
        "    Used for evaluating the trained `ANN` on adversarial versions of MNIST images.\n",
        "\n",
        "    Parameters\n",
        "    ==========\n",
        "\n",
        "    file_path: str\n",
        "      The path to the file from where the saved `BNN_Normal_Normal` is to be loaded.\n",
        "    input_shape: tuple\n",
        "    \n",
        "    Returns\n",
        "    =======\n",
        "\n",
        "    Tuple of the following,\n",
        "\n",
        "    preds: list containing tf.Tensor\n",
        "      Predictions made by neural network.\n",
        "    y_test: tf.Tensor\n",
        "      The target labels.\n",
        "    total_loss: tf.float32\n",
        "    uncertainty: tf.float32\n",
        "    \"\"\"\n",
        "    _, (x_test, y_test) = fashion_mnist.load_data()\n",
        "    inputs = []\n",
        "    for i in range(x_test.shape[0]):\n",
        "      inputs.append(x_test[i].flatten())\n",
        "    x_test = np.asarray(inputs)\n",
        "    x_test.reshape((x_test.shape[0], 784))\n",
        "    x_test = np.array(x_test, np.float32)\n",
        "    x_test = x_test / 126.\n",
        "    y_test = tf.one_hot(y_test, 10)\n",
        "    model = BNN_Normal_Normal(input_shape=input_shape)\n",
        "    model.load_weights(file_path)\n",
        "    preds, total_loss = model.get_loss(x_test, y_test, samples, 1., True)\n",
        "    uncertainty = shannon_entropy(preds, samples)\n",
        "    return preds, y_test, total_loss, uncertainty\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xOzrWuQAEyk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "outputId": "80847ca5-cb30-4a99-dfe7-e16e0993c2b1"
      },
      "source": [
        "preds, targets, _, uncertainty = test_mnist(prefix + \"weights/mnist/2020_08_09_08_27_21_674420\", (None, 784), \n",
        "                                            prefix + \"datasets/mnist_test_images\", prefix + \"datasets/mnist_test_labels\",\n",
        "                                            False, True)\n",
        "acc = 0\n",
        "oods = 0\n",
        "test_points = 10000\n",
        "for i in range(test_points):\n",
        "  is_ood = False\n",
        "  curr_output = tf.math.argmax(preds[0][i])\n",
        "  for j in range(1, samples):\n",
        "    if curr_output != tf.math.argmax(preds[j][i]):\n",
        "      is_ood = True\n",
        "      break\n",
        "  if is_ood:\n",
        "    oods += 1\n",
        "  else:\n",
        "      if curr_output == tf.math.argmax(targets[i]):\n",
        "        acc += 1\n",
        "print(\"Accuracy: \", acc/(test_points/100))\n",
        "print(\"OOD Inputs: \", oods)\n",
        "print(\"Uncertainty: \", uncertainty)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-4-31551de7919f>:69: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.add_weight` method instead.\n",
            "Loaded cached data from ./drive/My Drive/bnn/datasets/mnist_test_images_cache.npy\n",
            "Loaded cached data from ./drive/My Drive/bnn/datasets/mnist_test_labels_cache.npy\n",
            "Accuracy:  94.01\n",
            "OOD Inputs:  527\n",
            "Uncertainty:  tf.Tensor(0.011013616, shape=(), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDPatJelA7pw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "6e3fe32d-a645-4d4e-8b33-85d08537d24a"
      },
      "source": [
        "preds, targets, _, uncertainty = test_fmnist(prefix + \"weights/mnist/2020_08_09_08_27_21_674420\", (None, 784))\n",
        "acc = 0\n",
        "oods = 0\n",
        "test_points = 10000\n",
        "for i in range(test_points):\n",
        "  is_ood = False\n",
        "  curr_output = tf.math.argmax(preds[0][i])\n",
        "  for j in range(1, samples):\n",
        "    if curr_output != tf.math.argmax(preds[j][i]):\n",
        "      is_ood = True\n",
        "      break\n",
        "  if is_ood:\n",
        "    oods += 1\n",
        "  else:\n",
        "      if curr_output == tf.math.argmax(targets[i]):\n",
        "        acc += 1\n",
        "print(\"Accuracy: \", acc/(test_points/100))\n",
        "print(\"OOD Inputs: \", oods)\n",
        "print(\"Uncertainty: \", uncertainty)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 1s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "8192/5148 [===============================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n",
            "Accuracy:  3.43\n",
            "OOD Inputs:  3108\n",
            "Uncertainty:  tf.Tensor(0.044517577, shape=(), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}